Задание 1. Попробуйте изменить параметры нейронной сети работающей с датасетом imdb либо нейронной сети работающей airline-passengers(она прилагается вместе с датасетом к уроку в виде отдельного скрипта) так, чтобы улучшить ее точность. Приложите анализ.

результаты на старте:
Результат при тестировании: 0.3935762643814087
Тестовая точность: 0.8357599973678589

попробую изменить batch_size
batch_size = 100
лучше
Результат при тестировании: 0.3780427873134613
Тестовая точность: 0.8331999778747559

batch_size = 200
еще лучше
Результат при тестировании: 0.36509811878204346
Тестовая точность: 0.8438799977302551

попробовала поискать другой вариант для loss 
все плохо, наилучший вариант тот, который по-умолчанию - BinaryCrossentropy

loss = CategoricalCrossentropy
стало хуже
Результат при тестировании: 5.960464477539063e-08
Тестовая точность: 0.4934000074863434

плохо
loss = MeanSquaredError
Результат при тестировании: 0.11849802732467651
Тестовая точность: 0.8393999934196472

плохо
loss = MeanAbsolutePercentageError
Результат при тестировании: 6053.1923828125
Тестовая точность: 0.5

плохо
loss = MeanAbsoluteError
Результат при тестировании: 0.2146565467119217
Тестовая точность: 0.7916399836540222

плохо
loss = Huber
Результат при тестировании: 0.05837937816977501
Тестовая точность: 0.8380399942398071


попробовала поискать оптимайзер пока с дефолтными параметрами
по-умолчанию стоял Adam

лучший результат
optimizer = keras.optimizers.RMSprop()
Результат при тестировании: 0.3753525912761688
Тестовая точность: 0.8387200236320496

второй сверху
optimizer=keras.optimizers.Nadam()
Результат при тестировании: 0.3711957037448883
Тестовая точность: 0.8375599980354309


все остальное сильно хуже
optimizer=keras.optimizers.optimizers.Adadelta()
Результат при тестировании: 0.6929336190223694
Тестовая точность: 0.5166800022125244


optimizer=keras.optimizers.Adagrad()
Результат при тестировании: 0.6932175755500793
Тестовая точность: 0.4963200092315674


optimizer=keras.optimizers.Ftrl()
Результат при тестировании: 0.6931471228599548
Тестовая точность: 0.5


optimizer=keras.optimizers.SGD()
Результат при тестировании: 0.6931816935539246
Тестовая точность: 0.498879998922348


если попробовать поменять параметры оптимайзера с лучшим результатом
как было
optimizer = keras.optimizers.RMSprop()
Результат при тестировании: 0.3753525912761688
Тестовая точность: 0.8387200236320496


learning_rate=0.01
результат лучше, но точность ниже
Результат при тестировании: 0.4187251031398773
Тестовая точность: 0.7973600029945374


learning_rate=0.0001
результат еще лучше, но точность еще ниже
Результат при тестировании: 0.5851870775222778
Тестовая точность: 0.699400007724762


learning_rate=0.0001 + 5 эпох
результат лучше чем исходный, но точность пониже. поставлю еще больше эпох
Результат при тестировании: 0.4138140380382538
Тестовая точность: 0.8169199824333191

поставлю еще больше эпох
стало сильно лучше. и результат высокий и точность достаточно высокая.
если поставить еще больше, будет еще лучше
learning_rate=0.0001 + 20 эпох
Результат при тестировании: 0.4226941764354706
Тестовая точность: 0.8244799971580505


Итого:
результат и точность улучшают:
1) optimizer = keras.optimizers.RMSprop
2) learning_rate=0.0001 + большое количеcтво эпох
3) loss = keras.losses.BinaryCrossentropy


Задание 2. Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.

Увеличение BATCH_SIZE не улучшает результат.

Когда итераций мало и эпох мало, у меня текст содержит много повторов.
Пример, 1 итерация и 5 эпох
works on down the mouse to the mouse to the mouse to the mouse to the mouse to the mouse to the mouse to the m

Когда эпох много текст получше, даже если итераций мало.
Пример, 1 итерация и 20 эпох:
ice. of course, and the mouse to see it as if you didnt sleep the rabbit at the tried to say it as if you didn


когда итераций хотя-бы 3 или больше и эпох можно меньше, я взяла 5, текст получается получше.
Результат:
-ful soup! she said to the thing it was a little bitter, and the mock turtle in the white rabbit had not a mea


Я поставила много итераций, 5 эпох и получила более-менее осмысленный текст без повторов.

GRU, 25 итераций, 5 эпох
talking about it, and then alice did not like to be to down, what it meant to the cook took the hookah out of


LSTM, 25 итераций, 5 эпох
queen, but she had not at all a pit-chad hear her in the door of this agreement and help preserve free hot lik


SimpleRNN, 25 итераций, 5 эпох
opportunity of the white rabbit was a great hurry. oh down a time the mock turtle replied all the while the pi
